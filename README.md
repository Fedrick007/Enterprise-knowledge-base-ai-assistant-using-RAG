# ğŸ“š Enterprise RAG AI Assistant

A production-ready **Retrieval-Augmented Generation (RAG) AI Assistant** built using Large Language Models (LLMs), LangChain, FAISS, and FastAPI.

This system allows users to upload PDF documents and ask context-aware questions. It retrieves relevant information using vector search and generates accurate responses using OpenAI models.

---

## ğŸš€ Features

- End-to-End Retrieval-Augmented Generation (RAG) Pipeline  
- PDF Document Ingestion  
- Intelligent Text Chunking Strategy  
- OpenAI Embeddings Generation  
- FAISS Vector Database for Semantic Search  
- Context-Aware Question Answering  
- REST API using FastAPI  
- Modular and Scalable Architecture  
- Docker-Ready Deployment  
- Source Citation Support  
- Hallucination Reduction via Retrieval Grounding  

---

## ğŸ—ï¸ System Architecture

1. Load PDF documents from `data/` folder  
2. Split documents into chunks  
3. Generate embeddings using OpenAI  
4. Store embeddings in FAISS vector database  
5. Accept user question via API  
6. Retrieve relevant chunks using semantic search  
7. Pass retrieved context to LLM  
8. Generate final grounded response  

---

## ğŸ“‚ Project Structure

```
Enterprise-knowledge-base-ai-assistant-using-RAG/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ rag_pipeline.py
â”‚   â”œâ”€â”€ document_loader.py
â”‚   â”œâ”€â”€ vector_store.py
â”‚   â”œâ”€â”€ schemas.py
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ (Add PDF files here)
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .env
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ Tech Stack

- Python  
- FastAPI  
- LangChain  
- OpenAI API  
- FAISS (Vector Database)  
- Pydantic  
- Uvicorn  
- Docker  

---

## ğŸ”§ Installation & Setup

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/your-username/Enterprise-knowledge-base-ai-assistant-using-RAG.git
cd Enterprise-knowledge-base-ai-assistant-using-RAG
```

---

### 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate      # Linux / Mac
venv\Scripts\activate         # Windows
```

---

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

### 4ï¸âƒ£ Configure Environment Variables

Create a `.env` file in the root directory:

```
OPENAI_API_KEY=your_openai_api_key_here
```

---

### 5ï¸âƒ£ Add PDF Documents

Place your PDF files inside the `data/` folder.

---

## â–¶ï¸ Run the Application

```bash
uvicorn app.main:app --reload
```

Open Swagger UI:

```
http://127.0.0.1:8000/docs
```

Use the `/ask` endpoint to submit questions.

---

## ğŸ³ Docker Setup

### Build Docker Image

```bash
docker build -t rag-ai .
```

### Run Container

```bash
docker run -p 8000:8000 rag-ai
```

---

## ğŸ“Œ Example API Request

**POST** `/ask`

```json
{
  "question": "What are the key responsibilities mentioned in the document?"
}
```

### Example Response

```json
{
  "answer": "The key responsibilities include...",
  "sources": ["data/sample.pdf"]
}
```

---

## ğŸ“ˆ Engineering Highlights

- Designed modular RAG architecture  
- Implemented document chunking & embeddings  
- Built FAISS-based vector search system  
- Integrated OpenAI LLM with retrieval pipeline  
- Developed FastAPI-based AI microservice  
- Containerized application using Docker  
- Structured scalable backend design  

---

## ğŸ¯ Resume Value

This project demonstrates:

- Large Language Models (LLMs)  
- Generative AI System Design  
- Retrieval-Augmented Generation (RAG)  
- Vector Databases & Embeddings  
- Backend API Development  
- Dockerized Deployment  
- Scalable AI Microservice Architecture  

---

## ğŸ”® Future Enhancements

- Persistent FAISS storage  
- Chat memory support  
- Multi-user authentication  
- Cloud deployment (AWS/GCP/Azure)  
- Hallucination evaluation framework  
- Open-source LLM integration  

---

## ğŸ‘¨â€ğŸ’» Author

Fedrick Samuel W - 
Software Engineer | Python Developer  

